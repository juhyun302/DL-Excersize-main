{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1720232461098
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\EL19\\Downloads\\DL-Excersize-main\\DL-Excersize-main\\notebooks\\DL3_20241006\\ch06\n",
            "c:\\Users\\EL19\\Downloads\\DL-Excersize-main\\DL-Excersize-main\\notebooks\\DL3_20241006\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.3009951707054044\n",
            "=== epoch:1, train acc:0.169, test acc:0.2 ===\n",
            "train loss:2.2991100233415636\n",
            "train loss:2.295114630251247\n",
            "train loss:2.2923013847589835\n",
            "train loss:2.28181496442957\n",
            "train loss:2.272549814280407\n",
            "train loss:2.257373319786762\n",
            "train loss:2.2367935640085936\n",
            "train loss:2.208127032241117\n",
            "train loss:2.2061761379457443\n",
            "train loss:2.1862217920151155\n",
            "train loss:2.123761093133693\n",
            "train loss:2.095038751080735\n",
            "train loss:2.0463288388391745\n",
            "train loss:1.9889324170022946\n",
            "train loss:1.939065297274247\n",
            "train loss:1.919276567937763\n",
            "train loss:1.7417117494476713\n",
            "train loss:1.7066466131143663\n",
            "train loss:1.600913327461842\n",
            "train loss:1.5314033466986587\n",
            "train loss:1.397171857941583\n",
            "train loss:1.341068254226274\n",
            "train loss:1.1898572689615852\n",
            "train loss:1.2978406983873754\n",
            "train loss:1.1554281132376745\n",
            "train loss:1.1079776919299897\n",
            "train loss:0.9890347918157826\n",
            "train loss:0.9543674972073275\n",
            "train loss:0.8604363117768723\n",
            "train loss:0.7361629700733404\n",
            "train loss:0.7640685068597619\n",
            "train loss:0.9030880656057719\n",
            "train loss:0.8619216386776024\n",
            "train loss:0.6764652010924854\n",
            "train loss:0.6734670488881273\n",
            "train loss:0.6645236544155745\n",
            "train loss:0.6558945008874337\n",
            "train loss:0.6428831699286799\n",
            "train loss:0.694859349033087\n",
            "train loss:0.5872502599064648\n",
            "train loss:0.655462925736087\n",
            "train loss:0.6513810685555262\n",
            "train loss:0.6300654061785729\n",
            "train loss:0.5812254634343956\n",
            "train loss:0.7499546873713001\n",
            "train loss:0.5493980417423825\n",
            "train loss:0.4805870975544647\n",
            "train loss:0.3828304317363464\n",
            "train loss:0.4392379805847177\n",
            "train loss:0.6979516432629174\n",
            "=== epoch:2, train acc:0.823, test acc:0.809 ===\n",
            "train loss:0.41156823302121237\n",
            "train loss:0.765747214391519\n",
            "train loss:0.523676771980396\n",
            "train loss:0.5115680880959638\n",
            "train loss:0.4659668117126117\n",
            "train loss:0.38094277800896975\n",
            "train loss:0.5354219739573225\n",
            "train loss:0.6349781827570413\n",
            "train loss:0.4090933859770622\n",
            "train loss:0.4600171862041285\n",
            "train loss:0.3438789159418061\n",
            "train loss:0.40775161940865673\n",
            "train loss:0.34881449835598294\n",
            "train loss:0.30344752138221986\n",
            "train loss:0.4195171268505921\n",
            "train loss:0.2935621252729251\n",
            "train loss:0.31938724762383475\n",
            "train loss:0.3106437719863338\n",
            "train loss:0.32276767407200807\n",
            "train loss:0.35494981849204904\n",
            "train loss:0.2821636680969728\n",
            "train loss:0.30772852795474215\n",
            "train loss:0.29397959965007714\n",
            "train loss:0.3883420186191387\n",
            "train loss:0.5003583478757206\n",
            "train loss:0.5357698746307543\n",
            "train loss:0.3211103768114177\n",
            "train loss:0.37879475177724875\n",
            "train loss:0.38800871675458043\n",
            "train loss:0.4449375084174499\n",
            "train loss:0.33519075160946266\n",
            "train loss:0.3170516016757883\n",
            "train loss:0.39102161618425063\n",
            "train loss:0.4120102528189418\n",
            "train loss:0.3807838127972044\n",
            "train loss:0.29780989514969963\n",
            "train loss:0.21404081887529522\n",
            "train loss:0.2424437423574323\n",
            "train loss:0.27070327209999023\n",
            "train loss:0.4275882746090531\n",
            "train loss:0.42789749769254243\n",
            "train loss:0.26790812200894165\n",
            "train loss:0.2827252358597446\n",
            "train loss:0.3078764979507112\n",
            "train loss:0.4485302687714691\n",
            "train loss:0.23965743634256434\n",
            "train loss:0.37450536862053263\n",
            "train loss:0.35230751807475463\n",
            "train loss:0.346915506786826\n",
            "train loss:0.3295065580176836\n",
            "=== epoch:3, train acc:0.888, test acc:0.877 ===\n",
            "train loss:0.26176496263268834\n",
            "train loss:0.19808607156449995\n",
            "train loss:0.4700167377511083\n",
            "train loss:0.3039161574829277\n",
            "train loss:0.21288776071114998\n",
            "train loss:0.3012980029543584\n",
            "train loss:0.296844900530712\n",
            "train loss:0.3821348337215571\n",
            "train loss:0.3910953022961818\n",
            "train loss:0.32083176159380566\n",
            "train loss:0.3511145017362983\n",
            "train loss:0.284497537765332\n",
            "train loss:0.270480811191683\n",
            "train loss:0.3992928354999228\n",
            "train loss:0.3813568026590329\n",
            "train loss:0.14686699250719382\n",
            "train loss:0.19099775885492845\n",
            "train loss:0.40631796768319745\n",
            "train loss:0.3017151682475886\n",
            "train loss:0.3443313853974773\n",
            "train loss:0.3209013166209935\n",
            "train loss:0.19431966203309411\n",
            "train loss:0.38718405991966137\n",
            "train loss:0.37202729890026753\n",
            "train loss:0.3076583947901139\n",
            "train loss:0.3130624249666452\n",
            "train loss:0.15736850564470736\n",
            "train loss:0.19497460644503684\n",
            "train loss:0.28274676423138007\n",
            "train loss:0.2973540780446729\n",
            "train loss:0.37955199123425615\n",
            "train loss:0.26550627966191875\n",
            "train loss:0.301743915033323\n",
            "train loss:0.2259459860155717\n",
            "train loss:0.25204405248374306\n",
            "train loss:0.26401133983439173\n",
            "train loss:0.37397578366117956\n",
            "train loss:0.2921212901089497\n",
            "train loss:0.21451005565452128\n",
            "train loss:0.31192645413190184\n",
            "train loss:0.214302360245666\n",
            "train loss:0.23669257076484673\n",
            "train loss:0.2058652686820267\n",
            "train loss:0.27997862006904173\n",
            "train loss:0.29525883912684425\n",
            "train loss:0.20759875614801118\n",
            "train loss:0.33573522869408295\n",
            "train loss:0.276593751056779\n",
            "train loss:0.1634798748670185\n",
            "train loss:0.2371459483519676\n",
            "=== epoch:4, train acc:0.902, test acc:0.902 ===\n",
            "train loss:0.24665433342671428\n",
            "train loss:0.22670885896446602\n",
            "train loss:0.2524504862090448\n",
            "train loss:0.3066702895385174\n",
            "train loss:0.20399737621750502\n",
            "train loss:0.2652613922031891\n",
            "train loss:0.16084512954728208\n",
            "train loss:0.41668081033678916\n",
            "train loss:0.2896004874639073\n",
            "train loss:0.1890060776005692\n",
            "train loss:0.296843457690209\n",
            "train loss:0.24149431061988821\n",
            "train loss:0.1700635619468744\n",
            "train loss:0.21025259287519318\n",
            "train loss:0.17218116316538487\n",
            "train loss:0.1874604039049415\n",
            "train loss:0.20840122265741332\n",
            "train loss:0.2128078167440782\n",
            "train loss:0.24419851055746428\n",
            "train loss:0.110217468089756\n",
            "train loss:0.2332212867705684\n",
            "train loss:0.19379665397082146\n",
            "train loss:0.1612227435925289\n",
            "train loss:0.3074978945289143\n",
            "train loss:0.12441729406041922\n",
            "train loss:0.112644003133554\n",
            "train loss:0.22023045591915472\n",
            "train loss:0.3219615068915832\n",
            "train loss:0.23961297933738376\n",
            "train loss:0.2879182384170824\n",
            "train loss:0.22547491838429998\n",
            "train loss:0.18348461124777743\n",
            "train loss:0.306646099467078\n",
            "train loss:0.1545505407207295\n",
            "train loss:0.17636485937709434\n",
            "train loss:0.21168222962961047\n",
            "train loss:0.18341095009158534\n",
            "train loss:0.2226246163199147\n",
            "train loss:0.10909850424463627\n",
            "train loss:0.16384100600152318\n",
            "train loss:0.16863969075723065\n",
            "train loss:0.13827548443216733\n",
            "train loss:0.16854487846824776\n",
            "train loss:0.2717755102923589\n",
            "train loss:0.21494780723436763\n",
            "train loss:0.27249983938646566\n",
            "train loss:0.07810030063698567\n",
            "train loss:0.18238565717751862\n",
            "train loss:0.11990164621124429\n",
            "train loss:0.2718282386233208\n",
            "=== epoch:5, train acc:0.928, test acc:0.91 ===\n",
            "train loss:0.30664731038319754\n",
            "train loss:0.3112999765283537\n",
            "train loss:0.21043851981204376\n",
            "train loss:0.13051488881588116\n",
            "train loss:0.21095000603085926\n",
            "train loss:0.22391256524630102\n",
            "train loss:0.19951206784875494\n",
            "train loss:0.5049998756981855\n",
            "train loss:0.24174862947737694\n",
            "train loss:0.16552141891611122\n",
            "train loss:0.19530352663828\n",
            "train loss:0.23790867860152556\n",
            "train loss:0.14153758955273518\n",
            "train loss:0.11457899823945754\n",
            "train loss:0.15872748428334385\n",
            "train loss:0.16465844571805477\n",
            "train loss:0.12850352139293095\n",
            "train loss:0.17753057917031437\n",
            "train loss:0.16405968292490677\n",
            "train loss:0.2625237430624676\n",
            "train loss:0.2942139780077917\n",
            "train loss:0.16043861368435472\n",
            "train loss:0.22232521338304573\n",
            "train loss:0.1798858365399171\n",
            "train loss:0.21409817566387868\n",
            "train loss:0.2694415713095861\n",
            "train loss:0.21209793255913717\n",
            "train loss:0.18302216809885508\n",
            "train loss:0.11895647475460706\n",
            "train loss:0.1309968628361805\n",
            "train loss:0.24141640539305015\n",
            "train loss:0.22654570926111525\n",
            "train loss:0.20156470666946263\n",
            "train loss:0.09892943140150744\n",
            "train loss:0.11168197949745023\n",
            "train loss:0.2833562545026301\n",
            "train loss:0.17923482854334583\n",
            "train loss:0.17313301138401363\n",
            "train loss:0.2372277981529808\n",
            "train loss:0.20293864322197283\n",
            "train loss:0.14917239522957676\n",
            "train loss:0.2179084783536638\n",
            "train loss:0.2153508884340002\n",
            "train loss:0.12200274966344578\n",
            "train loss:0.172864493373399\n",
            "train loss:0.1885417188154327\n",
            "train loss:0.18279971907969572\n",
            "train loss:0.1804401398687788\n",
            "train loss:0.1994421301693147\n",
            "train loss:0.1409222532361841\n",
            "=== epoch:6, train acc:0.927, test acc:0.912 ===\n",
            "train loss:0.22832361922125252\n",
            "train loss:0.18811898420892598\n",
            "train loss:0.12799019168426154\n",
            "train loss:0.10489873259604993\n",
            "train loss:0.12921488703324985\n",
            "train loss:0.2002041735039358\n",
            "train loss:0.1822071156480736\n",
            "train loss:0.07308234863545796\n",
            "train loss:0.15429804758007756\n",
            "train loss:0.171459192482425\n",
            "train loss:0.12109163481570281\n",
            "train loss:0.1882630669455457\n",
            "train loss:0.2922848183673669\n",
            "train loss:0.17704345475547698\n",
            "train loss:0.1498921034508231\n",
            "train loss:0.15504679952413944\n",
            "train loss:0.10255275528546406\n",
            "train loss:0.14897874213469733\n",
            "train loss:0.1907492046658954\n",
            "train loss:0.09806155842778021\n",
            "train loss:0.12495369610189598\n",
            "train loss:0.2765467368994263\n",
            "train loss:0.20271116284938612\n",
            "train loss:0.09059461492487282\n",
            "train loss:0.1417078197768484\n",
            "train loss:0.13346059147539316\n",
            "train loss:0.14992361997072728\n",
            "train loss:0.14491621201334406\n",
            "train loss:0.13171414350619914\n",
            "train loss:0.12139930774893061\n",
            "train loss:0.1745934322878436\n",
            "train loss:0.13692583429146718\n",
            "train loss:0.22053513106986855\n",
            "train loss:0.1872128049604278\n",
            "train loss:0.1111945603369213\n",
            "train loss:0.2048346549107591\n",
            "train loss:0.2967239188227748\n",
            "train loss:0.16092352639939564\n",
            "train loss:0.18713542448123\n",
            "train loss:0.1634317550048056\n",
            "train loss:0.24004580979601847\n",
            "train loss:0.26079428038469143\n",
            "train loss:0.1707157511962394\n",
            "train loss:0.2053911745500829\n",
            "train loss:0.1969858427042758\n",
            "train loss:0.20540399116572783\n",
            "train loss:0.124211350476934\n",
            "train loss:0.147309297973177\n",
            "train loss:0.14782228872752218\n",
            "train loss:0.1273321573911784\n",
            "=== epoch:7, train acc:0.951, test acc:0.927 ===\n",
            "train loss:0.18525618511397168\n",
            "train loss:0.07470799185084093\n",
            "train loss:0.2077529233713239\n",
            "train loss:0.12160890751105016\n",
            "train loss:0.09727152419148581\n",
            "train loss:0.09807792489207116\n",
            "train loss:0.18896075969703702\n",
            "train loss:0.12029111036269385\n",
            "train loss:0.09776340183037265\n",
            "train loss:0.12996623173520272\n",
            "train loss:0.13970763818134324\n",
            "train loss:0.12829284587502213\n",
            "train loss:0.08363654756888446\n",
            "train loss:0.12357324806082844\n",
            "train loss:0.15693905254138601\n",
            "train loss:0.06724600767623083\n",
            "train loss:0.2046574081381527\n",
            "train loss:0.1438883749264413\n",
            "train loss:0.13263127057933133\n",
            "train loss:0.1598296342108981\n",
            "train loss:0.1470776578232594\n",
            "train loss:0.15773937872148103\n",
            "train loss:0.13189755965414093\n",
            "train loss:0.22490109998900545\n",
            "train loss:0.10463127634266385\n",
            "train loss:0.08262566276754255\n",
            "train loss:0.17779590664135728\n",
            "train loss:0.12620646418475295\n",
            "train loss:0.1271522971265241\n",
            "train loss:0.16041168001432976\n",
            "train loss:0.1383082451802373\n",
            "train loss:0.172670393615738\n",
            "train loss:0.047395809360961876\n",
            "train loss:0.17750417904023424\n",
            "train loss:0.1269400446534832\n",
            "train loss:0.1231315305425551\n",
            "train loss:0.12391869326507504\n",
            "train loss:0.13818667925621436\n",
            "train loss:0.08946684980906243\n",
            "train loss:0.1275898052394965\n",
            "train loss:0.10338853092436175\n",
            "train loss:0.18219846252586855\n",
            "train loss:0.13997000875482268\n",
            "train loss:0.09737481924613262\n",
            "train loss:0.16118530603990686\n",
            "train loss:0.1330136196503234\n",
            "train loss:0.11199019321693114\n",
            "train loss:0.08533292529659313\n",
            "train loss:0.08650650804558085\n",
            "train loss:0.09235086274008232\n",
            "=== epoch:8, train acc:0.948, test acc:0.939 ===\n",
            "train loss:0.22687436250483312\n",
            "train loss:0.07005233487557343\n",
            "train loss:0.22002235548843674\n",
            "train loss:0.08979703331580001\n",
            "train loss:0.1713318657599512\n",
            "train loss:0.1472522475766965\n",
            "train loss:0.10421913147130542\n",
            "train loss:0.127973714722966\n",
            "train loss:0.10551578838223057\n",
            "train loss:0.06667001976207913\n",
            "train loss:0.11329190105395451\n",
            "train loss:0.09264301936445812\n",
            "train loss:0.06396600965624687\n",
            "train loss:0.07053859591896314\n",
            "train loss:0.10195427811532545\n",
            "train loss:0.150830881813288\n",
            "train loss:0.09729334842833305\n",
            "train loss:0.14058972046491422\n",
            "train loss:0.09506474189161386\n",
            "train loss:0.07672426871008596\n",
            "train loss:0.0711117523857523\n",
            "train loss:0.10665700050267027\n",
            "train loss:0.19713605129978404\n",
            "train loss:0.19268843578176584\n",
            "train loss:0.118964123476875\n",
            "train loss:0.051944759861447955\n",
            "train loss:0.14012986687993859\n",
            "train loss:0.05464728697391667\n",
            "train loss:0.09857290980830355\n",
            "train loss:0.05376591065629034\n",
            "train loss:0.10411493575710778\n",
            "train loss:0.0831588183922784\n",
            "train loss:0.05171619956387324\n",
            "train loss:0.07455382734781081\n",
            "train loss:0.14496201290100055\n",
            "train loss:0.07908719479693016\n",
            "train loss:0.11987447186183803\n",
            "train loss:0.10876042907028627\n",
            "train loss:0.18783062890449362\n",
            "train loss:0.10417821816037687\n",
            "train loss:0.16653506042263022\n",
            "train loss:0.21421696119661346\n",
            "train loss:0.08348897991731087\n",
            "train loss:0.03335289357387251\n",
            "train loss:0.18961623211761286\n",
            "train loss:0.08093053884012331\n",
            "train loss:0.11223903074931889\n",
            "train loss:0.12215136290934808\n",
            "train loss:0.07568272116541763\n",
            "train loss:0.0833275838937655\n",
            "=== epoch:9, train acc:0.959, test acc:0.945 ===\n",
            "train loss:0.03151851451293151\n",
            "train loss:0.08634664485310259\n",
            "train loss:0.15902935829062884\n",
            "train loss:0.05961791097172536\n",
            "train loss:0.057200638241836764\n",
            "train loss:0.10215614580068921\n",
            "train loss:0.06624229721756197\n",
            "train loss:0.08551570148592345\n",
            "train loss:0.0917545882163876\n",
            "train loss:0.1243707889108268\n",
            "train loss:0.08870279197605221\n",
            "train loss:0.09771899743693613\n",
            "train loss:0.10875275293654331\n",
            "train loss:0.08207358831727855\n",
            "train loss:0.1310033934939475\n",
            "train loss:0.05255308012803024\n",
            "train loss:0.10045320023783097\n",
            "train loss:0.11185752042801553\n",
            "train loss:0.0790879449199763\n",
            "train loss:0.12558452034075998\n",
            "train loss:0.05109222323096584\n",
            "train loss:0.0949072600388818\n",
            "train loss:0.05928166889620973\n",
            "train loss:0.10186832175429247\n",
            "train loss:0.04007119263644048\n",
            "train loss:0.1454915627311278\n",
            "train loss:0.07255505741182425\n",
            "train loss:0.06068112124428361\n",
            "train loss:0.06507265930725267\n",
            "train loss:0.12994721018526323\n",
            "train loss:0.10809590963360137\n",
            "train loss:0.10325385925930078\n",
            "train loss:0.07411210209026847\n",
            "train loss:0.07999726272293685\n",
            "train loss:0.04559271666975098\n",
            "train loss:0.15555545450339955\n",
            "train loss:0.057702431569079594\n",
            "train loss:0.015140009480178341\n",
            "train loss:0.07287194163343594\n",
            "train loss:0.04091670513665505\n",
            "train loss:0.06436754480634899\n",
            "train loss:0.03257217416436717\n",
            "train loss:0.04723533612393525\n",
            "train loss:0.08196018725991078\n",
            "train loss:0.07721948408211023\n",
            "train loss:0.06280992430774682\n",
            "train loss:0.034241769213839965\n",
            "train loss:0.23783270218602354\n",
            "train loss:0.048314279885312394\n",
            "train loss:0.08644601924117733\n",
            "=== epoch:10, train acc:0.962, test acc:0.948 ===\n",
            "train loss:0.052827123820573224\n",
            "train loss:0.07746919143743888\n",
            "train loss:0.1409573456243747\n",
            "train loss:0.12358510079087132\n",
            "train loss:0.07365845335025567\n",
            "train loss:0.056571382062868436\n",
            "train loss:0.030946025517298737\n",
            "train loss:0.06396605821960963\n",
            "train loss:0.08751154469134788\n",
            "train loss:0.06784553708760642\n",
            "train loss:0.07745430507047348\n",
            "train loss:0.07179347685307583\n",
            "train loss:0.06038310380675891\n",
            "train loss:0.055054215560567006\n",
            "train loss:0.043695212061637394\n",
            "train loss:0.11348361735532031\n",
            "train loss:0.06260121175970455\n",
            "train loss:0.16001532872075291\n",
            "train loss:0.060264719618918654\n",
            "train loss:0.039394451150528316\n",
            "train loss:0.03815453330281664\n",
            "train loss:0.05488437383753569\n",
            "train loss:0.09272665853859513\n",
            "train loss:0.07424197097011526\n",
            "train loss:0.0657185250515724\n",
            "train loss:0.0417204003222511\n",
            "train loss:0.08519693184740831\n",
            "train loss:0.11871292239894796\n",
            "train loss:0.11782674613182902\n",
            "train loss:0.068658311539251\n",
            "train loss:0.047335228315202975\n",
            "train loss:0.06940323949763018\n",
            "train loss:0.10423844339208793\n",
            "train loss:0.11838201211965516\n",
            "train loss:0.05811414751522784\n",
            "train loss:0.06555482800260116\n",
            "train loss:0.036750546875287206\n",
            "train loss:0.09676376389136446\n",
            "train loss:0.043649870463171654\n",
            "train loss:0.09928231324578103\n",
            "train loss:0.034389434788315895\n",
            "train loss:0.10527605214336669\n",
            "train loss:0.037461928881542456\n",
            "train loss:0.0718859354571355\n",
            "train loss:0.08391406852374192\n",
            "train loss:0.0427960531757058\n",
            "train loss:0.04137135229940391\n",
            "train loss:0.12571060683313265\n",
            "train loss:0.06487502888264383\n",
            "train loss:0.023196179874652972\n",
            "=== epoch:11, train acc:0.967, test acc:0.946 ===\n",
            "train loss:0.0802737699200321\n",
            "train loss:0.13650499312656858\n",
            "train loss:0.12135255468935897\n",
            "train loss:0.056690985622933086\n",
            "train loss:0.04536333272786905\n",
            "train loss:0.07999669445695066\n",
            "train loss:0.07672483371001125\n",
            "train loss:0.05693253629244018\n",
            "train loss:0.07145121475605393\n",
            "train loss:0.09371894955666926\n",
            "train loss:0.09025557889550134\n",
            "train loss:0.06823448748394594\n",
            "train loss:0.08290186877729024\n",
            "train loss:0.047693836684634075\n",
            "train loss:0.045060420412536877\n",
            "train loss:0.05586076037419732\n",
            "train loss:0.026028440048843183\n",
            "train loss:0.026970977398771162\n",
            "train loss:0.0784830450323801\n",
            "train loss:0.09674877401670236\n",
            "train loss:0.07206960918950472\n",
            "train loss:0.05300066635782908\n",
            "train loss:0.09109563176738998\n",
            "train loss:0.06412567682709364\n",
            "train loss:0.059072920284780575\n",
            "train loss:0.03584280360324455\n",
            "train loss:0.04209850615733976\n",
            "train loss:0.11124700633124618\n",
            "train loss:0.1178211126846157\n",
            "train loss:0.05603510763336965\n",
            "train loss:0.042491360090425784\n",
            "train loss:0.08901236240796156\n",
            "train loss:0.023966854527580947\n",
            "train loss:0.08013737223340563\n",
            "train loss:0.13714681617907093\n",
            "train loss:0.03798758575048419\n",
            "train loss:0.1423519004992591\n",
            "train loss:0.0430162311803002\n",
            "train loss:0.07884554007341378\n",
            "train loss:0.03984765696862186\n",
            "train loss:0.07440654027111228\n",
            "train loss:0.08589643889281925\n",
            "train loss:0.1372924248099601\n",
            "train loss:0.17539728668354954\n",
            "train loss:0.05089049729172203\n",
            "train loss:0.0720338325421477\n",
            "train loss:0.09829067439196498\n",
            "train loss:0.03912635714065762\n",
            "train loss:0.03908337015969218\n",
            "train loss:0.05290147708820604\n",
            "=== epoch:12, train acc:0.968, test acc:0.942 ===\n",
            "train loss:0.05969507167405961\n",
            "train loss:0.05403833735928993\n",
            "train loss:0.09357558625285795\n",
            "train loss:0.06474811873442914\n",
            "train loss:0.08219373010192958\n",
            "train loss:0.04460164974480176\n",
            "train loss:0.05407342072953458\n",
            "train loss:0.04168649582237699\n",
            "train loss:0.07224331678183873\n",
            "train loss:0.059041594976250576\n",
            "train loss:0.024511950066128586\n",
            "train loss:0.07313887013533689\n",
            "train loss:0.029176828453534198\n",
            "train loss:0.03704945540697632\n",
            "train loss:0.023705845771304423\n",
            "train loss:0.025952933263438397\n",
            "train loss:0.07404505567724776\n",
            "train loss:0.07648162212658796\n",
            "train loss:0.032280524214832636\n",
            "train loss:0.07086476238113287\n",
            "train loss:0.06646912468992125\n",
            "train loss:0.05735697427992923\n",
            "train loss:0.0688309997882524\n",
            "train loss:0.019300257101984845\n",
            "train loss:0.07640010443008736\n",
            "train loss:0.08980694238573583\n",
            "train loss:0.08041086116755639\n",
            "train loss:0.02186696503991119\n",
            "train loss:0.057056160197116325\n",
            "train loss:0.08848683542936873\n",
            "train loss:0.1052869029080516\n",
            "train loss:0.11087778454795025\n",
            "train loss:0.048763025814161615\n",
            "train loss:0.03725484087067458\n",
            "train loss:0.035904833255146824\n",
            "train loss:0.19486123349722825\n",
            "train loss:0.06347588888452713\n",
            "train loss:0.028480079994091922\n",
            "train loss:0.02299920881795256\n",
            "train loss:0.07433347829333097\n",
            "train loss:0.04048137371528491\n",
            "train loss:0.12815111211710997\n",
            "train loss:0.06567861523791567\n",
            "train loss:0.02951036870250945\n",
            "train loss:0.05542416718631245\n",
            "train loss:0.052746055801716964\n",
            "train loss:0.028213841638950365\n",
            "train loss:0.030495685585793023\n",
            "train loss:0.030613679233731116\n",
            "train loss:0.02440798693624413\n",
            "=== epoch:13, train acc:0.982, test acc:0.958 ===\n",
            "train loss:0.05632000886738468\n",
            "train loss:0.051637785363859205\n",
            "train loss:0.04031257925461627\n",
            "train loss:0.028512462522277904\n",
            "train loss:0.04837474889345793\n",
            "train loss:0.03352515060716344\n",
            "train loss:0.08689842344910687\n",
            "train loss:0.02081422382379998\n",
            "train loss:0.05936743664807337\n",
            "train loss:0.02462873747201198\n",
            "train loss:0.03898449401545097\n",
            "train loss:0.08784245084789143\n",
            "train loss:0.04072164457855948\n",
            "train loss:0.034176213875851257\n",
            "train loss:0.06971040363267172\n",
            "train loss:0.039458722747356914\n",
            "train loss:0.03768856840701197\n",
            "train loss:0.07183551257851599\n",
            "train loss:0.03636060678878332\n",
            "train loss:0.04468579868611869\n",
            "train loss:0.043449206184582805\n",
            "train loss:0.03183563215384746\n",
            "train loss:0.18895409679265654\n",
            "train loss:0.07121169072336325\n",
            "train loss:0.05181095744402401\n",
            "train loss:0.030372072475028114\n",
            "train loss:0.02416592044552588\n",
            "train loss:0.06504909715324964\n",
            "train loss:0.02061285395716856\n",
            "train loss:0.00992757334223609\n",
            "train loss:0.018561958867632573\n",
            "train loss:0.06978240900720371\n",
            "train loss:0.049059210112812474\n",
            "train loss:0.017958778486292043\n",
            "train loss:0.022134488825729326\n",
            "train loss:0.079214231666493\n",
            "train loss:0.1308445740462198\n",
            "train loss:0.02088674723786597\n",
            "train loss:0.08900628000988647\n",
            "train loss:0.06140505480095659\n",
            "train loss:0.03410777182103705\n",
            "train loss:0.12708480132440897\n",
            "train loss:0.07414601308233094\n",
            "train loss:0.019135542335680798\n",
            "train loss:0.04855350676821061\n",
            "train loss:0.060120853250453035\n",
            "train loss:0.06084358852721253\n",
            "train loss:0.022626116098450097\n",
            "train loss:0.08175852113933083\n",
            "train loss:0.02391275709337383\n",
            "=== epoch:14, train acc:0.981, test acc:0.951 ===\n",
            "train loss:0.07400576855614867\n",
            "train loss:0.056469516241916214\n",
            "train loss:0.04311531056823406\n",
            "train loss:0.020365042304946623\n",
            "train loss:0.02747275973500804\n",
            "train loss:0.06649387639803837\n",
            "train loss:0.018903955546764297\n",
            "train loss:0.01963229365112111\n",
            "train loss:0.04625000800698479\n",
            "train loss:0.031860117551847374\n",
            "train loss:0.025054958298352973\n",
            "train loss:0.0417322334813017\n",
            "train loss:0.01920021808303423\n",
            "train loss:0.02416060911046357\n",
            "train loss:0.1047518202973668\n",
            "train loss:0.049153247268575624\n",
            "train loss:0.05851472103453001\n",
            "train loss:0.02877528167939401\n",
            "train loss:0.0544391301921917\n",
            "train loss:0.02658521776383106\n",
            "train loss:0.04084229692623688\n",
            "train loss:0.04328397507917534\n",
            "train loss:0.04504322375018421\n",
            "train loss:0.03507693977619032\n",
            "train loss:0.03670840748416148\n",
            "train loss:0.0306370152586993\n",
            "train loss:0.022163435874798406\n",
            "train loss:0.030942173093303348\n",
            "train loss:0.03657848836942433\n",
            "train loss:0.03695516167157496\n",
            "train loss:0.024225274826125312\n",
            "train loss:0.033858268944949164\n",
            "train loss:0.02196788825027101\n",
            "train loss:0.039391234354501856\n",
            "train loss:0.03587798239583415\n",
            "train loss:0.03283225893348792\n",
            "train loss:0.020435677251151186\n",
            "train loss:0.03412281527660415\n",
            "train loss:0.012759373977114595\n",
            "train loss:0.030419948914630627\n",
            "train loss:0.027904144913790584\n",
            "train loss:0.07630385597613086\n",
            "train loss:0.04833762629295484\n",
            "train loss:0.018517843858209107\n",
            "train loss:0.08385556872930305\n",
            "train loss:0.008853129800041807\n",
            "train loss:0.07635177483689598\n",
            "train loss:0.020663702770062752\n",
            "train loss:0.04298364145398452\n",
            "train loss:0.03719076954250962\n",
            "=== epoch:15, train acc:0.988, test acc:0.963 ===\n",
            "train loss:0.06752971962877531\n",
            "train loss:0.04182291056270503\n",
            "train loss:0.014376301892721892\n",
            "train loss:0.024606691359250293\n",
            "train loss:0.027693670906576907\n",
            "train loss:0.03545998762893967\n",
            "train loss:0.03041630830710079\n",
            "train loss:0.05751595128368336\n",
            "train loss:0.06841383108236347\n",
            "train loss:0.026005305469725078\n",
            "train loss:0.009068751010784686\n",
            "train loss:0.021974289085121237\n",
            "train loss:0.041509904440501746\n",
            "train loss:0.04673656298843937\n",
            "train loss:0.013242480582762919\n",
            "train loss:0.02310943746419087\n",
            "train loss:0.02238523319718158\n",
            "train loss:0.02736321900956608\n",
            "train loss:0.025940986628043526\n",
            "train loss:0.02337237734267921\n",
            "train loss:0.0793542418570131\n",
            "train loss:0.02719592734822562\n",
            "train loss:0.025209569593285817\n",
            "train loss:0.03382345502895088\n",
            "train loss:0.029494643909509892\n",
            "train loss:0.03521882046487013\n",
            "train loss:0.02139501812155189\n",
            "train loss:0.0403526661047551\n",
            "train loss:0.03817922924813809\n",
            "train loss:0.014121326333994589\n",
            "train loss:0.03800294179693286\n",
            "train loss:0.02520353436407128\n",
            "train loss:0.08012792968368922\n",
            "train loss:0.01914309386665652\n",
            "train loss:0.01549946293141662\n",
            "train loss:0.03243292961766133\n",
            "train loss:0.07092385413596841\n",
            "train loss:0.0205068765348036\n",
            "train loss:0.028884156489566593\n",
            "train loss:0.01647419041368517\n",
            "train loss:0.0404841202803956\n",
            "train loss:0.023283688672058746\n",
            "train loss:0.026428269762683018\n",
            "train loss:0.03508410758931466\n",
            "train loss:0.014997736922367826\n",
            "train loss:0.01773331238199723\n",
            "train loss:0.04238912603112967\n",
            "train loss:0.01875423534635949\n",
            "train loss:0.01077940612925257\n",
            "train loss:0.010470037369717607\n",
            "=== epoch:16, train acc:0.988, test acc:0.955 ===\n",
            "train loss:0.023864716119460665\n",
            "train loss:0.02792158538073827\n",
            "train loss:0.06018772312097578\n",
            "train loss:0.00938440331313428\n",
            "train loss:0.033080822558312024\n",
            "train loss:0.010176598829182744\n",
            "train loss:0.01714410349648613\n",
            "train loss:0.028126978416320232\n",
            "train loss:0.05490745404073474\n",
            "train loss:0.033563908560757076\n",
            "train loss:0.027648725254315182\n",
            "train loss:0.02264625136974837\n",
            "train loss:0.014664346299852265\n",
            "train loss:0.037616027149167355\n",
            "train loss:0.011900664039204917\n",
            "train loss:0.04713123012517398\n",
            "train loss:0.023850061625030575\n",
            "train loss:0.013588906247336205\n",
            "train loss:0.017849616421974655\n",
            "train loss:0.023941747569323147\n",
            "train loss:0.053546949924062004\n",
            "train loss:0.08312664683773853\n",
            "train loss:0.02257406370252428\n",
            "train loss:0.021242142050177434\n",
            "train loss:0.029357384322814248\n",
            "train loss:0.026254599487677124\n",
            "train loss:0.05319220754000555\n",
            "train loss:0.07444952130265928\n",
            "train loss:0.015033103037749476\n",
            "train loss:0.01157690533237378\n",
            "train loss:0.01868404766922639\n",
            "train loss:0.026728927634664147\n",
            "train loss:0.01713452772786276\n",
            "train loss:0.022879120959489498\n",
            "train loss:0.028674196635278374\n",
            "train loss:0.03156744765112554\n",
            "train loss:0.04775204496294027\n",
            "train loss:0.020037642783660884\n",
            "train loss:0.02251350354777919\n",
            "train loss:0.0068449488975665185\n",
            "train loss:0.012057346165698772\n",
            "train loss:0.07476062611524789\n",
            "train loss:0.031130647903884975\n",
            "train loss:0.012402326194037035\n",
            "train loss:0.02117751712323923\n",
            "train loss:0.02391849022973789\n",
            "train loss:0.016954222735085293\n",
            "train loss:0.0241919371676423\n",
            "train loss:0.04578203582908556\n",
            "train loss:0.03653310200810907\n",
            "=== epoch:17, train acc:0.987, test acc:0.95 ===\n",
            "train loss:0.034544615651985994\n",
            "train loss:0.029766212716447606\n",
            "train loss:0.016183068674587967\n",
            "train loss:0.029527012430707034\n",
            "train loss:0.030493206510905747\n",
            "train loss:0.014289415676287633\n",
            "train loss:0.047325420905278755\n",
            "train loss:0.019190346544231896\n",
            "train loss:0.03363141598984823\n",
            "train loss:0.03107849736216569\n",
            "train loss:0.009049617191379752\n",
            "train loss:0.04419251319015121\n",
            "train loss:0.013019383401313794\n",
            "train loss:0.01690018903325773\n",
            "train loss:0.017583223674098234\n",
            "train loss:0.03267901763395166\n",
            "train loss:0.008981548211240028\n",
            "train loss:0.04569063368450342\n",
            "train loss:0.0462105524013349\n",
            "train loss:0.03464283340292784\n",
            "train loss:0.009862471666331108\n",
            "train loss:0.027474110748366466\n",
            "train loss:0.012658989529413371\n",
            "train loss:0.012019937450531733\n",
            "train loss:0.016754524537425625\n",
            "train loss:0.014012907246697124\n",
            "train loss:0.01914634455193005\n",
            "train loss:0.03041971073756922\n",
            "train loss:0.027516514475191288\n",
            "train loss:0.017424435543714756\n",
            "train loss:0.01448093636427364\n",
            "train loss:0.02165322565416574\n",
            "train loss:0.01146718310770511\n",
            "train loss:0.014435054176591184\n",
            "train loss:0.036812431977389516\n",
            "train loss:0.014146651788968812\n",
            "train loss:0.01935533902746264\n",
            "train loss:0.035445153980858304\n",
            "train loss:0.032322944171713615\n",
            "train loss:0.032359014633667825\n",
            "train loss:0.010110934384760318\n",
            "train loss:0.015307300326456831\n",
            "train loss:0.04412173400053056\n",
            "train loss:0.05523009184892753\n",
            "train loss:0.018779237654917864\n",
            "train loss:0.061770433239983706\n",
            "train loss:0.024823329672351947\n",
            "train loss:0.020122793753496904\n",
            "train loss:0.041558928965422105\n",
            "train loss:0.030051568584720835\n",
            "=== epoch:18, train acc:0.989, test acc:0.952 ===\n",
            "train loss:0.012335532076148032\n",
            "train loss:0.023564907922017195\n",
            "train loss:0.02134311687648624\n",
            "train loss:0.012794417268283039\n",
            "train loss:0.01598605906949404\n",
            "train loss:0.03478620406192778\n",
            "train loss:0.013860899706733787\n",
            "train loss:0.040259335766209346\n",
            "train loss:0.008456083495520648\n",
            "train loss:0.007129107192536565\n",
            "train loss:0.016392802577472877\n",
            "train loss:0.008528857160800898\n",
            "train loss:0.02608707766060641\n",
            "train loss:0.015560870746719786\n",
            "train loss:0.045693268985630135\n",
            "train loss:0.012710918990211002\n",
            "train loss:0.023659179878854842\n",
            "train loss:0.04729847228813713\n",
            "train loss:0.009465083017344339\n",
            "train loss:0.05222623301950922\n",
            "train loss:0.007868256370833573\n",
            "train loss:0.029341588405169917\n",
            "train loss:0.028203213400142264\n",
            "train loss:0.02213126480921963\n",
            "train loss:0.019832912091932386\n",
            "train loss:0.012295762172711474\n",
            "train loss:0.01978636821315423\n",
            "train loss:0.018060822020421592\n",
            "train loss:0.012199816076035059\n",
            "train loss:0.008552123154756418\n",
            "train loss:0.014870677888096453\n",
            "train loss:0.03136623276632061\n",
            "train loss:0.03254587394331223\n",
            "train loss:0.010846918073509342\n",
            "train loss:0.009328700218666167\n",
            "train loss:0.04642292394190929\n",
            "train loss:0.029562959131258632\n",
            "train loss:0.006332819898542039\n",
            "train loss:0.01402458741588544\n",
            "train loss:0.016162711576571002\n",
            "train loss:0.008704055588359265\n",
            "train loss:0.009724496983202695\n",
            "train loss:0.039194870961519225\n",
            "train loss:0.04616533531234247\n",
            "train loss:0.01728431300626708\n",
            "train loss:0.016370863428437788\n",
            "train loss:0.009329170409161107\n",
            "train loss:0.05930719220198892\n",
            "train loss:0.011676288921826324\n",
            "train loss:0.01921559991113742\n",
            "=== epoch:19, train acc:0.991, test acc:0.957 ===\n",
            "train loss:0.036586520642192505\n",
            "train loss:0.03526279471407272\n",
            "train loss:0.007454269870482762\n",
            "train loss:0.012115026294241939\n",
            "train loss:0.00961309928609676\n",
            "train loss:0.011596768193617178\n",
            "train loss:0.024416406430840417\n",
            "train loss:0.008503184673923764\n",
            "train loss:0.020590695812838348\n",
            "train loss:0.03447588645278556\n",
            "train loss:0.014888027892540708\n",
            "train loss:0.06139852756738473\n",
            "train loss:0.01865296617467793\n",
            "train loss:0.02044684339233565\n",
            "train loss:0.020993800658532145\n",
            "train loss:0.03143302229697738\n",
            "train loss:0.02694316318475062\n",
            "train loss:0.010459222034085622\n",
            "train loss:0.015339885616914468\n",
            "train loss:0.018278428307428103\n",
            "train loss:0.011577562589525867\n",
            "train loss:0.02297990084152847\n",
            "train loss:0.022656385998250816\n",
            "train loss:0.005588986416466147\n",
            "train loss:0.008023441990910962\n",
            "train loss:0.024469482077222344\n",
            "train loss:0.010912241221151905\n",
            "train loss:0.007589507444874664\n",
            "train loss:0.029370234351054698\n",
            "train loss:0.01192761709965344\n",
            "train loss:0.010610415519018604\n",
            "train loss:0.03923127855950487\n",
            "train loss:0.013358676965655975\n",
            "train loss:0.03395131937993292\n",
            "train loss:0.021818167119489792\n",
            "train loss:0.008108416100164216\n",
            "train loss:0.042847061780627055\n",
            "train loss:0.015844425543369407\n",
            "train loss:0.034496498363404446\n",
            "train loss:0.009584037219409236\n",
            "train loss:0.06270390417899761\n",
            "train loss:0.011134382247658468\n",
            "train loss:0.013004502217538539\n",
            "train loss:0.005143796226624004\n",
            "train loss:0.007185066059119865\n",
            "train loss:0.01187520569203614\n",
            "train loss:0.016981979519260493\n",
            "train loss:0.012553492021609785\n",
            "train loss:0.008932010453861201\n",
            "train loss:0.00914658511297323\n",
            "=== epoch:20, train acc:0.991, test acc:0.961 ===\n",
            "train loss:0.006903749238582308\n",
            "train loss:0.015178938104670718\n",
            "train loss:0.008762335348381067\n",
            "train loss:0.022055434293517515\n",
            "train loss:0.010190204391165494\n",
            "train loss:0.013147243020642203\n",
            "train loss:0.01119184126530855\n",
            "train loss:0.007298780188436566\n",
            "train loss:0.006437653740400895\n",
            "train loss:0.011605118334578723\n",
            "train loss:0.0072970515005665705\n",
            "train loss:0.008609853484611853\n",
            "train loss:0.00839113683840329\n",
            "train loss:0.015607267918286696\n",
            "train loss:0.0042292481089681614\n",
            "train loss:0.004328148677961118\n",
            "train loss:0.005825639518435308\n",
            "train loss:0.01733125281391407\n",
            "train loss:0.016788577238951913\n",
            "train loss:0.008295498161286241\n",
            "train loss:0.024183758024256527\n",
            "train loss:0.002936137542217756\n",
            "train loss:0.01402154348345396\n",
            "train loss:0.014034578686081694\n",
            "train loss:0.008683471233905156\n",
            "train loss:0.01660470749022149\n",
            "train loss:0.011402975149174994\n",
            "train loss:0.008073337371246866\n",
            "train loss:0.015546231121391044\n",
            "train loss:0.014558743876086612\n",
            "train loss:0.012186185553796263\n",
            "train loss:0.005540148051891914\n",
            "train loss:0.036628149483792276\n",
            "train loss:0.03508878396820007\n",
            "train loss:0.005304778553977786\n",
            "train loss:0.009359809806459314\n",
            "train loss:0.018545933152868555\n",
            "train loss:0.005763124097025776\n",
            "train loss:0.013311289392181267\n",
            "train loss:0.009877347812603361\n",
            "train loss:0.013642415296286847\n",
            "train loss:0.003939172670595939\n",
            "train loss:0.008618080927508646\n",
            "train loss:0.0040880374121764354\n",
            "train loss:0.003931431984705269\n",
            "train loss:0.020196026792684382\n",
            "train loss:0.010893853787366244\n",
            "train loss:0.007908675199128102\n",
            "train loss:0.010739057734262787\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.96\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os, sys\n",
        "print(os.getcwd())\n",
        "current_dir = os.path.dirname(os.getcwd())\n",
        "print(current_dir)\n",
        "os.chdir(current_dir)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from ch06.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "#  \n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "#      .\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test [:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "#  \n",
        "network. save_params (\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "#  \n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer. train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer. test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
